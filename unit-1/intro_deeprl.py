# -*- coding: utf-8 -*-
"""intro-DeepRL.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VND2ykEAdsvYpWJt684-FbdDzpgWGj0M

### Install dependencies and create a virtual screen ðŸ”½
The first step is to install the dependencies, weâ€™ll install multiple ones.

- `gym[box2D]`: Contains the LunarLander-v2 environment ðŸŒ› (we use `gym==0.21`)
- `stable-baselines3[extra]`: The deep reinforcement learning library.
- `huggingface_sb3`: Additional code for Stable-baselines3 to load and upload models from the Hugging Face ðŸ¤— Hub.

To make things easier, we created a script to install all these dependencies.
"""

import warnings
import time 

import gym
import torch
from huggingface_hub import \
    notebook_login  # To log to our Hugging Face account to be able to upload models to the Hub.
from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub
from pyvirtualdisplay import Display
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.evaluation import evaluate_policy

warnings.filterwarnings("ignore")


def check_out_cuda():
    print("CUDA:", torch.cuda.is_available())
    print(torch.cuda.device(0))
    print(int(2e5))


def create_virtual_screen():
    # Virtual display
    virtual_display = Display(visible=0, size=(1400, 900))
    virtual_display.start()


def train_agent(model_name: str, environment_name: str, n_envs: int, n_steps: int, batch_size: int, n_epochs: int, total_timesteps):

    # create vectorized environment ()
    env = make_vec_env(environment_name, n_envs=n_envs)

    # create an Agent
    model = PPO(
        policy='MlpPolicy',
        env=env,
        n_steps=n_steps,
        batch_size=batch_size,
        n_epochs=n_epochs,
        gamma=0.999,
        gae_lambda=0.98,
        ent_coef=0.01,
        verbose=1)

    # Train it for n timesteps
    model.learn(total_timesteps=total_timesteps)
    # Save the model
    model.save(model_name)

    return model


def eval_agent(model, n_eval_episodes):

    # evaluate
    eval_env = gym.make("LunarLander-v2")

    # return mean reward and standard desviation (by 10 episodes and deterministic)
    mean_reward, std_reward = evaluate_policy(
        model, eval_env, n_eval_episodes=n_eval_episodes, deterministic=True)

    return mean_reward, std_reward


def main():

    # check out dependencies
    check_out_cuda()
    create_virtual_screen()

    # train and eval
    print("\n---------- Start RL Agent training -----------\n")

    start = time.time()
    model = train_agent(model_name="ppo-tunning-v2", environment_name="LunarLander-v2", n_envs=16,
                        n_steps=1000, batch_size=80, n_epochs=5, total_timesteps=2000000)
    mean_reward, std_reward = eval_agent(model=model, n_eval_episodes=10)
    end = time.time()

    total_time = end - start
    print("\n---------- Finished RL Agent training -----------\n")
    print("\n training time: "+ str(total_time))

    # metrics
    print("\n RL Agent metrics\n")
    print(f"mean_reward={mean_reward:.2f} +/- {std_reward}")


if __name__ == "__main__":
    main()
